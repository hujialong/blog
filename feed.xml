<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>茶绿茶行</title>
    <link href="https://blog.hjlhust.com/feed.xml" rel="self" />
    <link href="https://blog.hjlhust.com" />
    <updated>2021-07-21T12:18:59+08:00</updated>
    <author>
        <name>hujl</name>
    </author>
    <id>https://blog.hjlhust.com</id>

    <entry>
        <title>在Splunk中对数据进行脱敏</title>
        <author>
            <name>hujl</name>
        </author>
        <link href="https://blog.hjlhust.com/zai-splunkzhong-dui-shu-ju-jin-xing-tuo-min.html"/>
        <id>https://blog.hjlhust.com/zai-splunkzhong-dui-shu-ju-jin-xing-tuo-min.html</id>

        <updated>2021-07-21T12:18:59+08:00</updated>
            <summary>
                <![CDATA[
                    在我的很多客户都问过，如何在Splunk中对数据进行脱敏。通常这么做的目的是为了保护个人隐私信息或者符合安全法规的要求。在这篇博客中，我们会讨论在Splunk中对数据进行脱敏的一些方法。 在这篇博客中，我们所有操作都基于以下数据来实现： sourcetype=vendor_sales sample_log: [15/Dec/2016:07:36:56] VendorID=5069 Code=A AcctID=4397877757846973 [15/Dec/2016:07:21:15] VendorID=3108 Code=M AcctID=6347267702474393 其中我们需要将AcctID的前12为隐藏起来，只保留最后4位置 Splunk可以在index phase的时候使用SEDCMD对数据进行脱敏，基于以上实验数据，我们可以用以下配置来实现： props.conf [vendor_sales] SEDCMD-lacct = s/AcctID=\d{12}(\d{4})/AcctID=xxxxxxxxxxxx\1/g 这样Splunk在index数据之前会对AcctID进行脱敏，脱敏后的结果为： sample_log-Masked: [15/Dec/2016:07:36:56] VendorID=5069 Code=A AcctID=xxxxxxxxxxxx6973 [15/Dec/2016:07:21:15] VendorID=3108 Code=M AcctID=xxxxxxxxxxxx4393 TRANSFORMS 在Splunk中同时可以通过transform来实现： 在props.conf中定义transform： props.conf [vendor_sales] TRANSFORMS-anonymize = AcctID-anonymizer 同时在transforms.conf文件中定义脱敏的正则： transforms.conf [AcctID-anonymizer] REGEX = (?m)^(.*)AcctID=\d{12}+(\d{4}.*)$ FORMAT&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p>在我的很多客户都问过，如何在Splunk中对数据进行脱敏。通常这么做的目的是为了保护个人隐私信息或者符合安全法规的要求。在这篇博客中，我们会讨论在Splunk中对数据进行脱敏的一些方法。</p>
<h3 id="实验数据">实验数据</h3>
<p>在这篇博客中，我们所有操作都基于以下数据来实现：</p>
<pre><code class="language-log">sourcetype=vendor_sales
sample_log:
[15/Dec/2016:07:36:56] VendorID=5069 Code=A AcctID=4397877757846973
[15/Dec/2016:07:21:15] VendorID=3108 Code=M AcctID=6347267702474393
</code></pre>
<p>其中我们需要将AcctID的前12为隐藏起来，只保留最后4位置</p>
<h3 id="sedcmd">SEDCMD</h3>
<p>Splunk可以在index phase的时候使用SEDCMD对数据进行脱敏，基于以上实验数据，我们可以用以下配置来实现：</p>
<p><code>props.conf</code></p>
<pre><code class="language-conf">[vendor_sales]
SEDCMD-lacct = s/AcctID=\d{12}(\d{4})/AcctID=xxxxxxxxxxxx\1/g
</code></pre>
<p>这样Splunk在index数据之前会对AcctID进行脱敏，脱敏后的结果为：</p>
<pre><code class="language-log">sample_log-Masked:
[15/Dec/2016:07:36:56] VendorID=5069 Code=A AcctID=xxxxxxxxxxxx6973
[15/Dec/2016:07:21:15] VendorID=3108 Code=M AcctID=xxxxxxxxxxxx4393
</code></pre>
<h3 id="transforms">TRANSFORMS</h3>
<p>在Splunk中同时可以通过transform来实现：
在<code>props.conf</code>中定义transform： <code>props.conf</code></p>
<pre><code class="language-conf">[vendor_sales]
TRANSFORMS-anonymize = AcctID-anonymizer
</code></pre>
<p>同时在transforms.conf文件中定义脱敏的正则：</p>
<p><code>transforms.conf</code></p>
<pre><code class="language-conf">[AcctID-anonymizer]
REGEX = (?m)^(.*)AcctID=\d{12}+(\d{4}.*)$
FORMAT = $1AcctID=xxxxxxxxxxxx$2
DEST_KEY = _raw
</code></pre>
<p>这两种办法都是在index之前对数据进行脱敏，在Splunk平台是看到的是脱敏后的结果。可是往往有时候用户会有其他的一下情况出现：</p>
<ul>
<li>一部分有权限的人能够看到敏感数据</li>
<li>没有权限的人员只能看到脱敏后的数据 这个时候，如果在index之前就脱敏，那就无法实现了。下面将介绍在Splunk平台如何给予用户权限对数据进行脱敏。</li>
</ul>
<h3 id="基于用户权限的数据脱敏">基于用户权限的数据脱敏</h3>
<ul>
<li>同时转发两份数据到index，其中一份是没有脱敏的数据，另外一个脱敏后的数据，分别放在不通的index里面。（这种方式会消耗License）然后使用权限控制，让不通权限的用户访问不通的index。</li>
<li>运行一个schedule search，对数据进行脱敏，并生产summary index。同时让没有权限查看敏感数据的用户只能看到summary index的数据</li>
</ul>
<p>脱敏SPL样例：</p>
<pre><code class="language-spl">index=&quot;sales&quot; AcctID=* | rex field=AcctID mode=sed s/(\d{12})\d{4}/\1xxxx/g | table Vendor VendorID VendorCountry VendorCity Code AcctID productId product_name price sale_price
</code></pre>
<h3 id="参考正则表达式">参考正则表达式</h3>
<ul>
<li><p>信用卡/银行卡</p>
<pre><code class="language-regex">[creditcard-anonymizer]
REGEX=(?ms)(.*)\b(?:4[0-9]{8}(?:[0-9]{3})?|5[1-5][0-9]{10}|6(?:011|5[0-9]{2})[0-9]{8}|3[47][0-9]{9}|3(?:0[0-5]|[68][0-9])[0-9]{7}|(?:2131|1800|35\d{3})\d{7})(\d{4}\b.*)
FORMAT= $1###SCRUBBED###$2
DEST_KEY = _raw
</code></pre>
</li>
<li><p>ccpartner</p>
<pre><code class="language-regex">(?:4[0-9]{12}(?:[0-9]{3})?|5[1-5][0-9]{14}|6(?:011|5[0-9][0-9])[0-9]{12}|3[47][0-9]{13}|3(?:0[0-5]|[68][0-9])[0-9]{11}|(?:2131|1800|35\d{3})\d{11})
</code></pre>
</li>
</ul>

            ]]>
        </content>
    </entry>
    <entry>
        <title>Too small to match seekptr checksum</title>
        <author>
            <name>hujl</name>
        </author>
        <link href="https://blog.hjlhust.com/too-small-to-match-seekptr-checksum.html"/>
        <id>https://blog.hjlhust.com/too-small-to-match-seekptr-checksum.html</id>

        <updated>2021-07-21T12:09:02+08:00</updated>
            <summary>
                <![CDATA[
                    用Splunk Monitor文件（xml、html等）的时候，如果文件开始的256bytes是一样的话，Splunk默认会认为是一个文件，从而不会input进index。 用Splunk Monitor文件（xml、html等）的时候，如果文件开始的256bytes是一样的话，Splunk默认会认为是一个文件，从而不会input进index。我们会在$SPLUNK_HOME/var/log/splunkd.log中看到如下的报错： splunkd.log:09-22-2017 01:30:04.522 +0000 ERROR TailReader - File will not be read, is too small to match seekptr checksum (file=/home/ubuntu/logs/json-bowman-&lt;myserver&gt;1-bowman-worker_search-1.log). Last time we saw this initcrc, filename was different. You may wish to use larger initCrcLen for this sourcetype, or a CRC&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p>用Splunk Monitor文件（xml、html等）的时候，如果文件开始的256bytes是一样的话，Splunk默认会认为是一个文件，从而不会input进index。</p>
<h3 id="问题描述">问题描述</h3>
<p>用Splunk Monitor文件（xml、html等）的时候，如果文件开始的256bytes是一样的话，Splunk默认会认为是一个文件，从而不会input进index。我们会在$SPLUNK_HOME/var/log/splunkd.log中看到如下的报错：</p>
<pre><code class="language-shell">splunkd.log:09-22-2017 01:30:04.522 +0000 ERROR TailReader - File will not be read, is too small to match seekptr checksum (file=/home/ubuntu/logs/json-bowman-&lt;myserver&gt;1-bowman-worker_search-1.log). Last time we saw this initcrc, filename was different. You may wish to use larger initCrcLen for this sourcetype, or a CRC salt on this source. Consult the documentation or file a support case online at http://www.splunk.com/page/submit_issue for more info.
</code></pre>
<h3 id="分析">分析</h3>
<p>Splunk默认通过校验文件头的256bytes，来判断文件是否是新文件，其控制Splunk对文件进行校验的参数在<code>inputs.conf</code>文件中：</p>
<pre><code class="language-conf">initCrcLength = 256     # default value:256,     
</code></pre>
<h3 id="workaround">Workaround</h3>
<p>在<code>inputs.conf</code>文件中加入<code>crcSalt = &lt;SOURCE&gt;</code>,这样<code>inputs.conf</code>文件看起来会是这样：</p>
<pre><code class="language-conf">[monitor://\\dhcpsrv\dhcp$]
disabled = 0
followTail = 0
host = dhcpsrv
index = default
sourcetype = ms_dhcpd
_whitelist = DhcpSrvLog.(Sun|Mon|Tue|Wed|Thu|Fri|Sat)$
crcSalt = &lt;SOURCE&gt;
</code></pre>

            ]]>
        </content>
    </entry>
    <entry>
        <title>Collect ldapsearch result to index</title>
        <author>
            <name>hujl</name>
        </author>
        <link href="https://blog.hjlhust.com/collect-ldapsearch-result-to-index.html"/>
        <id>https://blog.hjlhust.com/collect-ldapsearch-result-to-index.html</id>

        <updated>2021-07-21T12:09:10+08:00</updated>
            <summary>
                <![CDATA[
                    当我用collect命令将ldapsearch的结果存入到index的时候遇到一些问题，collect命令并不能很好切分ldapsearch的结果。在collect之前使用table命令将能很好解决这个问题。 我使用以下SPL从ldap上将用户账号信息存放在ldap_summary上，以便日后通过查询使用。（也可以输出为csv，通过lookup来使用，这个不在这里讨论） | ldapsearch domain=&quot;xxx-xx.in&quot; search=&quot;(&amp;(sAMAccoutName=*))&quot; basedn=&quot;OU=AD Account,DC=xxx-xx,DC=in&quot; attrs=&quot;sAMAccountName,cn,sn,giveName,emplyeeID,title,department,description,mail,telephoneNumber,memberOt,distinguishedName&quot; | collect index=ldap_summary 结果有一部分按行切分为独立的event，大部分，随机的多行（最多有3000行）合并为一个event，😢 通过多次测试，发现只要在collect命令之前，使用table命令，可以很好的解决这个问题。具体为什么会这样目前还不太清楚。 需改后的SPL如下： | ldapsearch domain=&quot;xxx-xx.in&quot; search=&quot;(&amp;(sAMAccoutName=*))&quot; basedn=&quot;OU=AD Account,DC=xxx-xx,DC=in&quot; attrs=&quot;sAMAccountName,cn,sn,giveName,emplyeeID,title,department,description,mail,telephoneNumber,memberOt,distinguishedName&quot; | table sAMAccou ntName,cn,sn,giveName,emplyeeID,title,department,description,mail,telephoneNumber,memberOt,distinguishedName | collect index=ldap_summary 
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p>当我用collect命令将ldapsearch的结果存入到index的时候遇到一些问题，collect命令并不能很好切分ldapsearch的结果。在collect之前使用table命令将能很好解决这个问题。</p>
<h3 id="问题描述">问题描述</h3>
<p>我使用以下SPL从ldap上将用户账号信息存放在ldap_summary上，以便日后通过查询使用。（也可以输出为csv，通过lookup来使用，这个不在这里讨论）</p>
<pre><code class="language-spl">| ldapsearch domain=&quot;xxx-xx.in&quot; search=&quot;(&amp;(sAMAccoutName=*))&quot; basedn=&quot;OU=AD Account,DC=xxx-xx,DC=in&quot; attrs=&quot;sAMAccountName,cn,sn,giveName,emplyeeID,title,department,description,mail,telephoneNumber,memberOt,distinguishedName&quot;
| collect index=ldap_summary
</code></pre>
<p>结果有一部分按行切分为独立的event，大部分，随机的多行（最多有3000行）合并为一个event，😢</p>
<h3 id="workaround">Workaround</h3>
<p>通过多次测试，发现只要在collect命令之前，使用table命令，可以很好的解决这个问题。具体为什么会这样目前还不太清楚。</p>
<p>需改后的SPL如下：</p>
<pre><code class="language-spl">| ldapsearch domain=&quot;xxx-xx.in&quot; search=&quot;(&amp;(sAMAccoutName=*))&quot; basedn=&quot;OU=AD Account,DC=xxx-xx,DC=in&quot; attrs=&quot;sAMAccountName,cn,sn,giveName,emplyeeID,title,department,description,mail,telephoneNumber,memberOt,distinguishedName&quot;
| table sAMAccou
ntName,cn,sn,giveName,emplyeeID,title,department,description,mail,telephoneNumber,memberOt,distinguishedName
| collect index=ldap_summary
</code></pre>

            ]]>
        </content>
    </entry>
</feed>
